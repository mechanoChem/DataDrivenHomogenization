[DEFAULT]

[TEST]
DataNormOption = 0
DropData = 0
# will determined by the config file location
root = 
DataFile = ../../data/cnn-1dns-base-free-energy/*.vtk,
#AllFields = F11, F22, F12, F21
#LabelFields = F12, F22
NumberOfInspect = 0
AllFields = 
LabelFields = 
DerivativeFields = 
LabelScale = 100
LabelShift = 0
FeatureShift = 
DataNormalization = 1
SplitRatio = 0.8, 0.1, 0.1

[KBNN]
MatlabLabel = 
# Leave the (), for model selection, and trained model programming after conference 
LabelShiftingModels = # manually given: should read from old input files, and should also set std, mean for all the dataset, instead of features
OldShiftFeatures = 
OldShiftMean = 
OldShiftStd = 
OldShiftLabelScale = 

EmbeddingModels = 
OldEmbedInputSelect = 
OldEmbedFeatures = 
OldEmbedMean = 
OldEmbedStd = 
OldEmbedLabelScale = 

[MODEL]
OutputLayer = Dense
OutputLayerActivation = 
GaussNoise = 0
KRegL1 = 0
KRegL2 = 0
# selection of different model architect
# pure_DNN, pure_DNN_user, IDNN_user, KBNN_user_test,pure_CNN, CNN_autoencoder
ModelArchitect = CNN_supervise
#adam,sgd, adagrad, adamax, rmsprop, 
Optimizer  = adam
LearningRate = decay_exp, 0.001, 100, 0.92
Loss = my_mse_loss
Metrics = 
CallBacks = printdot, checkpoint
#,  reducelrplateau, tensorboard
HyperParameterSearch = N
SensitivityAnalysisNumber = 0
# HiddenLayerNumbers is determined by the following
ParameterID = 0
LayerName = Conv2D_3_3, MaxPooling2D_2_2, Conv2D_3_3, MaxPooling2D_2_2, Conv2D_3_3, MaxPooling2D_2_2, Conv2D_3_3, MaxPooling2D_2_2, Flatten
NodesList = 2, 0, 3, 0, 5, 0, 6, 0, 0
Activation = relu, relu, relu, relu, relu, relu, relu, relu, relu
Padding = same, same, same, same, same, same, same, same, same
EarlyStopPatience = 10
Epochs = 10000
BatchSize = 85
Verbose = 1
RepeatTrain = 1
KFoldTrain = 5

[FORMAT]
# file format, etc
CSVSep = ,
PrintStringForm = {:>20}

[OUTPUT]
InspectLayers = 0
NumImages = 0
#PlotFields = loss, mse, scatter, error, r_error, mae, r_error,t_loss, 
PlotFields = loss, scatter, label
TensorBoardDir = tmp_log/
TensorBoardFields = 
FinalModelSummary = restart/model_summary
NumGradientCalc = 0
#grad0, grad1, grad2, grad3, int1, plot_int1,, int1, grad0, int1, grad1
GradientOutput = grad0 
#eval_of_zeros, jacobian_of_zeros, grad2_of_zeros, grad1_of_zeros, eval_of_I, grad1_of_I, grad2_of_I, jacobian_of_I
SpecialOperation = 
#grad1_of_zeros
SpecialX = 

[RESTART]
SavedModel = 
RestartAtModel = 0
CheckPointDir = restart/
SavedCheckPoint = saved_weight/cp-10000.ckpt 
CheckPointPeriod = 100
RestartModel = N
RestartWeight = N

[HYPERPARAMETERS]
# list of parameters to study
HiddenLayerNumber = 1, 10
NodesList = 2, 32, 1
# softsign not easy
#elu, exponential, hard_sigmoid, linear, relu, selu, sigmoid, softmax, softplus, tanh 
# good for 1st derivatives: relu, elu, tanh, softplus, softmax, sigmoid, 
# good for 2nd derivatives:     , elu, tanh, softplus, softmax, sigmoid, 
# not good derivatives: exponential, hard_sigmoid, linear, selu(total wrong)
Activation = relu
#, softmax, sigmoid, tanh, elu 
LearningRate = 0.01

[PLOT]
# various setup for plottings
Option = default
