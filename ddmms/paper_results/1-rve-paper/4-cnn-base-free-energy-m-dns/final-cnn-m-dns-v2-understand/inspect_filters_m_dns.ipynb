{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.0.0-beta0\n",
      "/home/xiaoxuan/Dropbox/2-Codes/projects_python/ddmms/paper_results/1-rve-paper/4-cnn-base-free-energy-m-dns/final-cnn-m-dns-v2-understand\n",
      "... read ... configfile =  cnn-free-energy-m-dns-final.config\n",
      "old root is: <class 'str'> ['']\n",
      "...modifying.. DataFile from:  ../../data/cnn-m-dns-base-free-energy/*.vtk,\n",
      "...modifying.. DataFile to:  /home/xiaoxuan/Dropbox/2-Codes/projects_python/ddmms/paper_results/1-rve-paper/4-cnn-base-free-energy-m-dns/final-cnn-m-dns-v2-understand/../../data/cnn-m-dns-base-free-energy/*.vtk\n",
      "load_all_data\n",
      "*****************WARNING**********************:\n",
      "if have multiple VTK folder and it's the first time to load vtk and save numpy array. There is a\n",
      "potential bug, that after the 1st vtk folder, the following numpy array file is getting bigger \n",
      "and bigger, try to fix this bug next time!!!!!\n",
      "***********************************************\n",
      "load_all_data_from_vtk_database\n",
      "/home/xiaoxuan/Dropbox/2-Codes/projects_python/ddmms/paper_results/1-rve-paper/4-cnn-base-free-energy-m-dns/final-cnn-m-dns-v2-understand/../../data/cnn-m-dns-base-free-energy/*.vtk\n",
      "/home/xiaoxuan/Dropbox/2-Codes/projects_python/ddmms/paper_results/1-rve-paper/4-cnn-base-free-energy-m-dns/final-cnn-m-dns-v2-understand/../../data/cnn-m-dns-base-free-energy/*\n",
      "load saved numpy for the label folder\n",
      "load saved numpy for vtk folder\n",
      "all data :  (17000, 61, 61, 1)\n",
      "the label:  (17000,)\n",
      "all data :  tf.Tensor([17000    61    61     1], shape=(4,), dtype=int32)\n",
      "the label:  tf.Tensor([17000], shape=(1,), dtype=int32)\n",
      "...done with data loading\n",
      "***WARNING***: TF1.x only support label with 1 variable.\n",
      "successfully load saved kfold status files!\n",
      "In the fold:  0  with  train size:  12240 validation size:  3060\n",
      "...done with parameters\n",
      "input_shape: [61 61  1]\n",
      "checkpoint_dir for restart:  ./saved_weight\n",
      "latest checkpoint:  ./saved_weight/cp-10000.ckpt\n",
      "Successfully load weight:  ./saved_weight/cp-10000.ckpt\n",
      "--Decay in exponential rate: initial rate =  0.001 , decay steps =  100 , decay_rate =  0.92\n",
      "!!!! Caution: use Learning rate with care, there were occasions that tf1.13 should better performance on training. !!!\n",
      "Avail Optimizer:  ['adam', 'sgd', 'adadelta', 'gradientdescentoptimizer']\n",
      "Avail Loss:  ['mse', 'mae', 'sparse_categorical_crossentropy', 'binary_crossentropy']\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "WARNING:tensorflow:Entity <bound method CNN_user_supervise.call of <ddmms.models.CNN_user.CNN_user_supervise object at 0x7f075c3bfa90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CNN_user_supervise.call of <ddmms.models.CNN_user.CNN_user_supervise object at 0x7f075c3bfa90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method CNN_user_supervise.call of <ddmms.models.CNN_user.CNN_user_supervise object at 0x7f075c3bfa90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CNN_user_supervise.call of <ddmms.models.CNN_user.CNN_user_supervise object at 0x7f075c3bfa90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Unused attribute in object (root).input_layer: ['OBJECT_CONFIG_JSON']\n",
      "WARNING:tensorflow:Unused attribute in object (root).output_layer: ['OBJECT_CONFIG_JSON']\n",
      "WARNING:tensorflow:Unused attribute in object (root).encoder_layer.0: ['OBJECT_CONFIG_JSON']\n",
      "WARNING:tensorflow:Unused attribute in object (root).encoder_layer.1: ['OBJECT_CONFIG_JSON']\n",
      "WARNING:tensorflow:Unused attribute in object (root).encoder_layer.2: ['OBJECT_CONFIG_JSON']\n",
      "WARNING:tensorflow:Unused attribute in object (root).encoder_layer.3: ['OBJECT_CONFIG_JSON']\n",
      "WARNING:tensorflow:Unused attribute in object (root).encoder_layer.4: ['OBJECT_CONFIG_JSON']\n",
      "WARNING:tensorflow:Unused attribute in object (root).encoder_layer.5: ['OBJECT_CONFIG_JSON']\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "Train on 13600 samples, validate on 1700 samples\n",
      "Model: \"cnn_user_supervise\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  90        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  1230      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            multiple                  2176      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  1025      \n",
      "=================================================================\n",
      "Total params: 4,521\n",
      "Trainable params: 4,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load modules\n",
    "# for legacy python compatibility\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# # TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# disable GPU and only CPU and Warning etc\n",
    "import os, sys\n",
    "\n",
    "import datetime\n",
    "from SALib.analyze import sobol\n",
    "\n",
    "# from tensorflow import feature_column\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pathlib\n",
    "# import seaborn as sns\n",
    "\n",
    "import ddmms.help.ml_help as ml_help\n",
    "\n",
    "import ddmms.preprocess.ml_preprocess as ml_preprocess\n",
    "\n",
    "import ddmms.parameters.ml_parameters as ml_parameters\n",
    "import ddmms.parameters.ml_parameters_cnn as ml_parameters_cnn\n",
    "\n",
    "import ddmms.models.ml_models as ml_models\n",
    "import ddmms.models.ml_optimizer as ml_optimizer\n",
    "import ddmms.models.ml_loss as ml_loss\n",
    "\n",
    "import ddmms.postprocess.ml_postprocess as ml_postprocess\n",
    "import ddmms.math.ml_math as ml_math\n",
    "import ddmms.specials.ml_specials as ml_specials\n",
    "\n",
    "import ddmms.misc.ml_misc as ml_misc\n",
    "import ddmms.misc.ml_callbacks as ml_callbacks\n",
    "\n",
    "import ddmms.vis.ml_vis as ml_vis\n",
    "import ddmms.vis.ml_lrp as ml_lrp\n",
    "import ddmms.vis.ml_utils as ml_utils\n",
    "\n",
    "import ddmms.train.ml_kfold as ml_kfold\n",
    "import socket\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "print(os.getcwd())\n",
    "args = ml_help.sys_args()\n",
    "\n",
    "args.configfile = 'cnn-free-energy-m-dns-final.config'\n",
    "\n",
    "if (socket.gethostname() == 'pc256g'):\n",
    "    args.platform = 'cpu'\n",
    "else:\n",
    "    args.platform = 'gpu'\n",
    "args.inspect = 0\n",
    "args.debug = False\n",
    "args.verbose = 1\n",
    "args.show = 0\n",
    "\n",
    "ml_help.notebook_args(args)\n",
    "config = ml_preprocess.read_config_file(args.configfile, args.debug)\n",
    "# train_dataset, train_labels, val_dataset, val_labels, test_dataset, test_labels, test_derivative, train_stats = ml_preprocess.load_and_inspect_data(\n",
    "    # config, args)\n",
    "dataset, labels, derivative, train_stats = ml_preprocess.load_all_data(config, args)\n",
    "\n",
    "str_form = config['FORMAT']['PrintStringForm']\n",
    "epochs = int(config['MODEL']['Epochs'])\n",
    "batch_size = int(config['MODEL']['BatchSize'])\n",
    "verbose = int(config['MODEL']['Verbose'])\n",
    "n_splits = int(config['MODEL']['KFoldTrain'])\n",
    "\n",
    "the_kfolds = ml_kfold.MLKFold(n_splits, dataset)\n",
    "train_dataset, train_labels, val_dataset, val_labels, test_dataset, test_labels, test_derivative = the_kfolds.get_next_fold(\n",
    "    dataset, labels, derivative, final_data=True)\n",
    "\n",
    "#total_model_numbers = parameter.get_model_numbers()\n",
    "print(\"...done with parameters\")\n",
    "model_summary_list = []\n",
    "\n",
    "config['RESTART']['CheckPointDir'] = './saved_weight'\n",
    "config['MODEL']['ParameterID'] = ''\n",
    "checkpoint_dir = config['RESTART']['CheckPointDir'] + config['MODEL']['ParameterID']\n",
    "model_path = checkpoint_dir + '/' + 'model.h5'\n",
    "\n",
    "model = ml_models.build_model(config, train_dataset, train_labels)\n",
    "# https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model\n",
    "\n",
    "config['RESTART']['RestartWeight'] = 'Y'\n",
    "epochs = 0\n",
    "\n",
    "if (config['RESTART']['RestartWeight'].lower() == 'y'):\n",
    "    print('checkpoint_dir for restart: ', checkpoint_dir)\n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    print(\"latest checkpoint: \", latest)\n",
    "    # latest=\"/opt/scratch/ml/cnn-hyperelasticity-bvp-2d-conv/restart/cp-2000.ckpt\"\n",
    "    if (latest != None):\n",
    "        model.load_weights(latest)\n",
    "        print(\"Successfully load weight: \", latest)\n",
    "    else:\n",
    "        print(\"No saved weights, start to train the model from the beginning!\")\n",
    "        pass\n",
    "\n",
    "metrics = ml_misc.getlist_str(config['MODEL']['Metrics'])\n",
    "optimizer = ml_optimizer.build_optimizer(config)\n",
    "loss = ml_loss.build_loss(config)\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "callbacks = ml_callbacks.build_callbacks(config)\n",
    "# print(type(train_dataset))\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    train_labels,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(val_dataset, val_labels),    # or validation_split= 0.1,\n",
    "    verbose=verbose,\n",
    "    callbacks=callbacks)\n",
    "# print(tf.keras.backend.eval(optimizer.lr))\n",
    "# print(model.optimizer.lr.get_value())\n",
    "\n",
    "model.summary()\n",
    "# print(\"history: \" , history.history['loss'], history.history['val_loss'], history.history)\n",
    "\n",
    "label_scale = float(config['TEST']['LabelScale'])\n",
    "all_data = {'test_label': [], 'test_nn': [], 'val_label': [], 'val_nn': [], 'train_label': [], 'train_nn': []}\n",
    "\n",
    "test_nn = model.predict(test_dataset, verbose=0, batch_size=batch_size)\n",
    "val_nn = model.predict(val_dataset, verbose=0, batch_size=batch_size)\n",
    "train_nn = model.predict(train_dataset, verbose=0, batch_size=batch_size)\n",
    "\n",
    "if epochs > 0 :\n",
    "    for i in np.squeeze(test_nn):\n",
    "        all_data['test_nn'].append(i / label_scale)\n",
    "    for i in np.squeeze(val_nn):\n",
    "        all_data['val_nn'].append(i / label_scale)\n",
    "    for i in np.squeeze(train_nn):\n",
    "        all_data['train_nn'].append(i / label_scale)\n",
    "    \n",
    "    for i in test_labels:\n",
    "        all_data['test_label'].append(i / label_scale)\n",
    "    for i in val_labels:\n",
    "        all_data['val_label'].append(i / label_scale)\n",
    "    for i in train_labels:\n",
    "        all_data['train_label'].append(i / label_scale)\n",
    "    # print('all_data: ', all_data)\n",
    "    \n",
    "    import pickle\n",
    "    import time\n",
    "    now = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    pickle_out = open('all_data_' + now + '.pickle', \"wb\")\n",
    "    pickle.dump(all_data, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    pickle_out = open('history_' + now + '.pickle', \"wb\")\n",
    "    pickle.dump(history.history, pickle_out)\n",
    "    pickle_out.close()\n",
    "    print('save to: ', 'all_data_' + now + '.pickle', 'history_' + now + '.pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml_postprocess.inspect_cnn_features(model, config, test_dataset, savefig=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml_postprocess.inspect_cnn_features(model, config, test_dataset[20:50], savefig=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml_postprocess.inspect_cnn_features(model, config, test_dataset[300:400], savefig=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_cnn_features_output_each_filter(model, config, test_dataset, savefig=False, name=''):\n",
    "    num_images = int(config['OUTPUT']['NumImages'])\n",
    "    inspect_layers = ml_misc.getlist_int(config['OUTPUT']['InspectLayers'])\n",
    "\n",
    "    total_images = 0\n",
    "    for l0 in inspect_layers:\n",
    "        out1 = model.check_layer(test_dataset[0:1], l0)\n",
    "        total_images += tf.shape(out1[0]).numpy()[2]\n",
    "        print('total_images:', total_images)\n",
    "\n",
    "    if (int(np.sqrt(total_images)) * int(np.sqrt(total_images)) >=\n",
    "            total_images):\n",
    "        num_col = int(np.sqrt(total_images))\n",
    "    else:\n",
    "        num_col = int(np.sqrt(total_images)) + 1\n",
    "    num_row = num_col\n",
    "\n",
    "    for i0 in range(0, num_images):\n",
    "        plt.figure()\n",
    "        count = 0\n",
    "        for l0 in inspect_layers:\n",
    "            out1 = model.check_layer(test_dataset[i0:i0 + 1], l0)\n",
    "            img0 = out1[0]\n",
    "            shape0 = tf.shape(img0).numpy()\n",
    "            print('shape0: ', shape0)\n",
    "            for i in range(1,shape0[2] + 1):  # 2nd index is the feature numbers\n",
    "                count += 1\n",
    "                ax = plt.gca()\n",
    "                plt.imshow(out1[0, :, :, i - 1])  # tensor\n",
    "                print(out1)\n",
    "                plt.gray()\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "                if savefig:\n",
    "                    plt.savefig(name+str(l0)+'-'+str(i)+'.pdf', bbox_inches='tight', format='pdf')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_cnn_features(model, config, test_dataset, savefig=False, name=''):\n",
    "    num_images = int(config['OUTPUT']['NumImages'])\n",
    "    inspect_layers = ml_misc.getlist_int(config['OUTPUT']['InspectLayers'])\n",
    "\n",
    "    total_images = 0\n",
    "    for l0 in inspect_layers:\n",
    "        out1 = model.check_layer(test_dataset[0:1], l0)\n",
    "        total_images += tf.shape(out1[0]).numpy()[2]\n",
    "        print('total_images:', total_images)\n",
    "\n",
    "    if (int(np.sqrt(total_images)) * int(np.sqrt(total_images)) >=\n",
    "            total_images):\n",
    "        num_col = int(np.sqrt(total_images))\n",
    "    else:\n",
    "        num_col = int(np.sqrt(total_images)) + 1\n",
    "    num_row = num_col\n",
    "\n",
    "    for i0 in range(0, num_images):\n",
    "        plt.figure()\n",
    "        count = 0\n",
    "        for l0 in inspect_layers:\n",
    "            out1 = model.check_layer(test_dataset[i0:i0 + 1], l0)\n",
    "            #print(out1)\n",
    "            img0 = out1[0]\n",
    "            shape0 = tf.shape(img0).numpy()\n",
    "            print('shape0: ', shape0)\n",
    "            for i in range(1,shape0[2] + 1):  # 2nd index is the feature numbers\n",
    "                count += 1\n",
    "                ax = plt.subplot(num_col, num_row, count)\n",
    "                plt.imshow(out1[0, :, :, i - 1])  # tensor\n",
    "                plt.gray()\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "        if savefig:\n",
    "            plt.savefig(name+str(l0)+'.png', bbox_inches='tight', format='png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-afc9766cd281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OUTPUT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NumImages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OUTPUT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'InspectLayers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minspect_cnn_features_output_each_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'm-dns-image-l'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OUTPUT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'InspectLayers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minspect_cnn_features_output_each_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'm-dns-image-l'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "config['OUTPUT']['NumImages']='1'\n",
    "config['OUTPUT']['InspectLayers'] = '0'\n",
    "inspect_cnn_features_output_each_filter(model, config, test_dataset[300:400], savefig=True, name='m-dns-image-l')\n",
    "config['OUTPUT']['InspectLayers'] = '1'\n",
    "inspect_cnn_features_output_each_filter(model, config, test_dataset[300:400], savefig=True, name='m-dns-image-l')\n",
    "#config['OUTPUT']['InspectLayers'] = '3'\n",
    "#inspect_cnn_features(model, config, test_dataset[300:400], savefig=True, name='m-dns-image-l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config['OUTPUT']['NumImages']='1'\n",
    "config['OUTPUT']['InspectLayers'] = '0'\n",
    "inspect_cnn_features(model, config, test_dataset[350:400], savefig=True, name='m-dns-image-2-l')\n",
    "config['OUTPUT']['InspectLayers'] = '1'\n",
    "inspect_cnn_features(model, config, test_dataset[350:400], savefig=True, name='m-dns-image-2-l')\n",
    "config['OUTPUT']['InspectLayers'] = '3'\n",
    "inspect_cnn_features(model, config, test_dataset[350:400], savefig=True, name='m-dns-image-2-l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['OUTPUT']['NumImages']='1'\n",
    "config['OUTPUT']['InspectLayers'] = '0'\n",
    "inspect_cnn_features_output_each_filter(model, config, test_dataset[1000:1100], savefig=True, name='m-dns-image-3-l')\n",
    "config['OUTPUT']['InspectLayers'] = '1'\n",
    "inspect_cnn_features_output_each_filter(model, config, test_dataset[1000:1100], savefig=True, name='m-dns-image-3-l')\n",
    "#config['OUTPUT']['InspectLayers'] = '3'\n",
    "#inspect_cnn_features(model, config, test_dataset[1000:1100], savefig=True, name='m-dns-image-3-l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.load('m1.npy')\n",
    "config['OUTPUT']['NumImages']='1'\n",
    "config['OUTPUT']['InspectLayers'] = '0'\n",
    "inspect_cnn_features(model, config, m1, savefig=True, name='m-dns-image-4-l')\n",
    "config['OUTPUT']['InspectLayers'] = '1'\n",
    "inspect_cnn_features(model, config, m1, savefig=True, name='m-dns-image-4-l')\n",
    "config['OUTPUT']['InspectLayers'] = '3'\n",
    "inspect_cnn_features(model, config, m1, savefig=True, name='m-dns-image-4-l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['OUTPUT']['NumImages']='1'\n",
    "config['OUTPUT']['InspectLayers'] = '0'\n",
    "inspect_cnn_features(model, config, test_dataset[0:1100], savefig=True, name='m-dns-image-5-l')\n",
    "config['OUTPUT']['InspectLayers'] = '1'\n",
    "inspect_cnn_features(model, config, test_dataset[0:1100], savefig=True, name='m-dns-image-5-l')\n",
    "config['OUTPUT']['InspectLayers'] = '3'\n",
    "inspect_cnn_features(model, config, test_dataset[0:1100], savefig=True, name='m-dns-image-5-l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['OUTPUT']['NumImages']='1'\n",
    "config['OUTPUT']['InspectLayers'] = '0'\n",
    "inspect_cnn_features(model, config, test_dataset[20:1100], savefig=True, name='m-dns-image-6-l')\n",
    "config['OUTPUT']['InspectLayers'] = '1'\n",
    "inspect_cnn_features(model, config, test_dataset[20:1100], savefig=True, name='m-dns-image-6-l')\n",
    "config['OUTPUT']['InspectLayers'] = '3'\n",
    "inspect_cnn_features(model, config, test_dataset[20:1100], savefig=True, name='m-dns-image-6-l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config['OUTPUT']['NumImages']='1'\n",
    "#config['OUTPUT']['InspectLayers'] = '1'\n",
    "#for i in range(0, 500):\n",
    "#    inspect_cnn_features(model, config, test_dataset[i:i+2], savefig=True, name='m-dns-image-a'+str(i)+'-l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
