[DEFAULT]

[TEST]
# will determined by the config file location
root = 
DataFile = ../../data/kbnn-1-frame-1-dns-perturbed-free-energy-E/kbnn_all_csv_data.csv
NumberOfInspect = 20
AllFields = E11,E12,E22,Psi_me,P11,P12,P21,P22,F11,F12,F21,F22
#,vol_rectangle_p,vol_rectangle_m,len_c,len_s_r_p,len_s_r_m, 
LabelFields = Psi_me,P11,P12,P21,P22,F11,F12,F21,F22
DerivativeFields = 
LabelScale = 100000
LabelShift = 0
FeatureShift = 
DataNormalization = 1
SplitRatio = 0.65, 0.2, 0.15
DataNormOption = 0
DropData = 0

[KBNN]
MatlabLabel = 
# Leave the (), for model selection, and trained model programming after conference 
LabelShiftingModels = ../../2-dnn-base-free-energy-1dns/final-dnn-1dns/dnn-free-energy-1dns-final.config
# manually given: should read from old input files, and should also set std, mean for all the dataset, instead of features
OldShiftFeatures = vol_rectangle_p, vol_rectangle_m,len_c,len_s_r_p,len_s_r_m
OldShiftMean = 0.195798, 0.195384, 0.074566, 0.051366,0.055325
OldShiftStd = 0.014416, 0.017324, 0.033848, 0.014693, 0.016482
OldShiftLabelScale = 100
OldShiftDataNormOption = 3
OldShiftCNNSavedBaseFrameNumpyName =

EmbeddingModels = 
OldEmbedInputSelect = 
OldEmbedFeatures =  
OldEmbedMean = 
OldEmbedStd =  
OldEmbedLabelScale = 1

# should read from the old input files
#OldLabel = Psi_me
#OldLabelScale = 100

[MODEL]
OutputLayer = No
Padding = same
LayerName = Dense
OutputLayerActivation = 
KRegL1 = 0
KFoldTrain = 5
RepeatTrain = 1
# selection of different model architect
# pure_DNN, pure_DNN_user, IDNN_user, KBNN_user_test
#ModelArchitect = DNN_kregl1l2_gauss
ModelArchitect = user_DNN_kregl1l2_gauss_grad
#ModelArchitect = pure_DNN
#adam,sgd, adagrad, adamax, rmsprop, 
Optimizer  = adam
LearningRate = decay_exp, 0.001, 100, 0.7
Loss = my_mse_loss_with_grad
Metrics = 
 # printdot, earlystop,
CallBacks = printdot
#, reducelrplateau, tensorboard, checkpoint, 
HyperParameterSearch = N
SensitivityAnalysisNumber = 0
# HiddenLayerNumbers is determined by the following
NodesList = 32, 32, 32, 32
#linear, sigmoid, tanh, elu, relu, softmax, exponential, selu,  softplus, softsign,
Activation = softplus, softplus, softplus, softplus
EarlyStopPatience = 10
Epochs = 2000
BatchSize = 160
Verbose = 0
ParameterID = 
KRegL2 = 0.001
GaussNoise = 0.000

[FORMAT]
# file format, etc
CSVSep = ,
PrintStringForm = {:>20}

[OUTPUT]
InspectLayers = 0
NumImages = 0
#PlotFields = loss, mse, scatter, error, r_error, mae, r_error,t_loss, 
PlotFields = loss, scatter, label
TensorBoardDir = tmp_log/
TensorBoardFields = 
FinalModelSummary = restart/model_summary
NumGradientCalc = 5
#grad0, grad1, grad2, grad3, int1, plot_int1,, int1, grad0, int1, grad1
GradientOutput = grad0, grad1
#eval_of_zeros, jacobian_of_zeros, grad2_of_zeros, grad1_of_zeros, eval_of_I, grad1_of_I, grad2_of_I, jacobian_of_I
SpecialOperation = 
#eval_of_x, grad1_of_x 
#grad1_of_x
#grad1_of_zeros
## IGA input (1)
#SpecialX = 1.0019439912e+00, 8.8709359252e-03,-9.4049993498e-03,1.0018056069e+00
## IGA results (1): 1.9139262313e-01, -1.6875080825e-03, 1.1198635705e-03, 2.2882753362e-01
# NN (1): [0.19592944 0.06437865 0.01161645 0.21401472]
## IGA input (2)
#SpecialX = 1.0020000000e+00,5.8936136456e-03,-6.0705725611e-03,1.0018868000e+00
# IGA results (2): 2.0957858332e-01,2.4117189720e-04,-1.0228824732e-05,1.7837520544e-01,
# NN (2) : 0.2593925  -0.54407966 -0.00443107  0.31597227
## IGA input (3)
SpecialX = 
#1.0020000000e+00,7.5263722877e-03,-8.0403539764e-03,1.0018868000e+00
# IGA results (3): 3.2858531340e-01,-1.1069127717e-05,-3.9238937148e-05,1.1541883664e-01,
# NN (3) :  0.22209753 -0.16787107 -0.05668398  0.29001626

[RESTART]
SavedModel = 
CheckPointDir = restart/
CheckPointPeriod = 100
RestartModel = N
RestartWeight = N
RestartAtModel = 0

[HYPERPARAMETERS]
# list of parameters to study
HiddenLayerNumber = 1, 10
NodesList = 2, 128, 1
# softsign not easy
#elu, exponential, hard_sigmoid, linear, relu, selu, sigmoid, softmax, softplus, tanh 
# good for 1st derivatives: relu, elu, tanh, softplus, softmax, sigmoid, 
# good for 2nd derivatives:     , elu, tanh, softplus, softmax, sigmoid, 
# not good derivatives: exponential, hard_sigmoid, linear, selu(total wrong)
Activation = softplus
LearningRate = 0.1, 0.01, 0.001

[PLOT]
# various setup for plottings
Option = default
